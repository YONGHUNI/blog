{
  "hash": "706d1945c1f1de51a146667eadf51ebd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"How to boost a DBSCAN task\"\nsubtitle: \"Utilizing High Performance Library from Python\"\nauthor: \"Yonghun Suh\"\ndate: \"Sep. 9, 2025\"\ncategories: [Code]\nimage: https://raw.githubusercontent.com/wangyiqiu/dbscan-python/0.0.12-dev/compare.png\nformat: \n  html:\n    toc: true\n    number_sections: true\n    code-copy: true\n    code-fold: show\n    code-tools: true\n    code-overflow: scroll\n    code-link: true\n    number-sections: true\n    toc_depth: 3\n    lightbox: true\n#execute:\n#  freeze: true\n#comments: false\n---\n\n# Backdrop\n\n![Chat](data/background.jpg){width=80%}\n\n\n\n\n>[Frodo, 2:37 PM] I'm working on finding representative points for census tracts using nationwide residential building data, weighted by the population of each tract.<br>\n[Frodo, 2:38 PM] But DBSCAN is taking forever lol, so I'm running it in parallel.<br>\n[Frodo, 2:38 PM] Using something like the future package in R.<br>\n[Yong-Hun Suh, 2:41 PM] Yeah, DBSCAN’s time complexity isn’t great…<br>\n[Yong-Hun Suh, 4:47 PM] Are you still working on that (DBSCAN)?<br>\n[Yong-Hun Suh, 4:47 PM] If it’s a project you’re going to continue, I can give you a tip…<br>\n[Frodo, 4:53 PM] Just fixed the code and running it now haha, results should come out in a **few days** if it’s fast.<br>\n\n\n\nI was chatting with my colleague from grad school who said DBSCAN was running too slowly on their project.\n\n\n## **Analyzing complexity factors**\n\n\n## DBSCAN time complexity\n\nDBSCAN’s runtime is dominated by how you perform neighborhood (range) queries. The classic “it’s $O(n²)$” is only the worst case of a spectrum that depends on data distribution, dimensionality, the index used, and the radius parameter $ \\varepsilon $.\n\n---\n\n## Core steps and cost drivers\n\n- **Range queries:** For each point, find all neighbors within radius $ \\varepsilon $. Distance evaluation for one pair costs $O(d)$ in $d$-dimensional Euclidean space.\n- **Cluster expansion:** A queue-based flood-fill that repeatedly issues range queries starting from core points (points with at least $ \\text{minPts} $ neighbors within $ \\varepsilon $).\n\nAsymptotically, the number and cost of range queries dominate; the expansion logic is linear in the number of discovered neighbors but tied to the same range-query results.\n\n---\n\n## Complexity by neighborhood search strategy\n\n### 1) Brute-force (no index)\n- Per range query: compute distance to all points ⇒ $O(n \\cdot d)$.\n- One query per point in the simplest implementation ⇒ $O(n^2 \\cdot d)$.\n- Cluster expansion may reuse queries or cause repeats; asymptotically the bound remains $ \\Theta(n^2 \\cdot d) $ in the worst case.\n\n### 2) Space-partitioning trees (kd-tree, ball tree, R-tree-like)\n- Index build: typically $O(n \\log n)$ time, $O(n)$ space.\n- Range query cost:\n  - Best/average (well-behaved low–moderate $d$, balanced tree, moderate $ \\varepsilon $): $O(\\log n + k)$, where $k$ is the number of reported neighbors.\n  - Worst case (high $d$, large $ \\varepsilon $, or adversarial data): degenerates to $O(n)$.\n- Overall:\n  - Best/average: $O(n \\log n + \\sum \\limits_{i=1}^{n} (\\log n + k_i)) = O(n \\log n + K)$, where $K = \\sum k_i$ is total neighbor reports. If density per query is bounded, $K = O(n)$, giving $O(n \\log n)$ plus distance cost factor $O(d)$.\n  - Worst: $O(n^2 \\cdot d)$.\n\n### 3) Uniform grid (fixed-radius hashing) in low dimensions\n- Build grid hashing once: expected $O(n)$.\n- Range query: constant number of adjacent cells, expected $O(1 + k)$ in 2D/3D if $ \\varepsilon $ is aligned with cell size and data are not pathologically skewed.\n- Overall in practice: near-linear $O(n + K)$, again with distance cost $O(d)$, but this approach becomes brittle as $d$ grows.\n\n> In all cases, include the distance computation factor $O(d)$. For high dimensions, tree and grid pruning effectiveness collapses, pushing complexity toward $ \\Theta(n^2 \\cdot d) $.\n\n---\n\n## What influences the runtime\n\n- **Dimensionality $d$:** Distance costs scale with $O(d)$, and index pruning degrades with the curse of dimensionality, often turning tree queries into $O(n)$.\n- **Neighborhood radius $ \\varepsilon $:** Larger $ \\varepsilon $ increases average neighbor count $k$, raises $K=\\sum k_i$, and triggers more expansions; in the limit, most points neighbor each other ⇒ near $O(n^2 \\cdot d)$.\n- **Data distribution and density:** Well-separated, roughly uniform, low-density data favor subquadratic performance with indexes. Dense clusters or large connected components increase expansions and repeats.\n- **minPts:** Affects how many points become core (thus how much expansion occurs). It changes constants and practical behavior but not the worst-case big-O bound.\n- **Distance metric:** Non-Euclidean metrics can alter pruning efficacy and per-distance cost.\n- **Implementation details:** Caching of range queries, deduplication, and “expand only once per point” optimizations materially reduce constants.\n\n---\n\n## Practical scenarios\n\n| Scenario | Assumptions | Typical complexity |\n|---|---|---|\n| Brute-force baseline | Any $d$, no index | $ \\Theta(n^2 \\cdot d) $ |\n| Tree index, low–moderate $d$ | Balanced tree, moderate $ \\varepsilon $, bounded neighbor counts | $ O(n \\log n \\cdot d) $ |\n| Tree index, high $d$ or large $ \\varepsilon $ | Poor pruning, dense neighborhoods | $ \\Theta(n^2 \\cdot d) $ |\n| Grid index (2D/3D) | Well-chosen cell size, non-pathological data | Near $ O(n \\cdot d) $ |\n| Approximate NN/range | ANN structure (e.g., HNSW), approximate neighbors | Subquadratic wall-time; formal bounds vary |\n\nNotes:\n- The index build cost $O(n \\log n)$ (trees) or $O(n)$ (grid) is typically amortized across all range queries.\n- If each point’s neighbor count $k_i$ is bounded by a small constant on average, and pruning works, the total neighbor reports $K$ is $O(n)$, leading to near $O(n \\log n)$ behavior with trees.\n\n\n\n\n> But... What if you cannot reduce your dataset... nor change the parameter of the DBSCAN?\n\n\n\n## Solution...? At least for this case :)\n\n**Use the better algorithms that utilizes your hardware's full potential!**\n\nFor this case, I used `dbscan-python`, which is a high-performance parallel implementation of DBSCAN, based on the SIGMOD 2020 paper “[Theoretically Efficient and Practical Parallel DBSCAN](https://github.com/wangyiqiu/dbscan-python){target=\"_blank\"}.”\n\n\n\n\n\n# Experiments\n\n## Setups\n\n### Used R Packages\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dbscan'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'arrow'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:utils':\n\n    timestamp\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: future\n```\n\n\n:::\n:::\n\n\n\n### Python Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\n\n\n\nif (Sys.info()[[1]]==\"Windows\") {\n        \n    # For my Windows Environment\n    use_condaenv(\"C:/Users/dydgn/miniforge3/envs/dbscan/python.exe\")\n    \n    } else{\n    \n    \n    system(\"micromamba install -n baseline -c conda-forge dbscan -y\")\n\n    # For github actions to utilize CI/CD\n\n    use_condaenv(\"/home/runner/micromamba/envs/baseline/bin/python\", required = TRUE)   \n\n    \n}\n\n\n\nsys <- import(\"sys\")\n\nsys$executable\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"/home/runner/micromamba/envs/baseline/bin/python\"\n```\n\n\n:::\n:::\n\n\n\n\n## Generated Lab Dataset\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerate_random_semicircle <- function(center_x, center_y, radius, start_angle, end_angle, n_points = 100, noise = 0.08) {\n    \n      angles <- runif(n_points, min = start_angle, max = end_angle)\n      x <- center_x + radius * cos(angles) + rnorm(n_points, 0, noise)\n      y <- center_y + radius * sin(angles) + rnorm(n_points, 0, noise)\n      \n      return(as.data.table(list(x = x, y = y)))\n}\n\n\nset.seed(123)\n\n\nN <- 1e+5L\n\n\n# 첫 번째 반원\nsemicircle1 <- generate_random_semicircle(center_x = 0, center_y = 0.25, radius = 1,\n                                          start_angle = pi, end_angle = 2 * pi, n_points = N)\nsemicircle1[, group := \"Semicircle 1\"]\n\n# 두 번째 반원\nsemicircle2 <- generate_random_semicircle(center_x = 1, center_y = -0.25, radius = 1,\n                                          start_angle = 0, end_angle = pi, n_points = N)\nsemicircle2[, group := \"Semicircle 2\"]\n\n\n# 데이터 결합\ndf <- rbindlist(list(semicircle1, semicircle2))\n\n\n# 시각화\nggplot(df, aes(x = x, y = y, color = group)) +\n  geom_point(size = 2) +\n  coord_fixed(ratio = 1) +\n  theme_minimal() +\n  labs(title = \"Randomized Overlapping Semicircles\", x = NULL, y = NULL) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlab_data <- df[, !c(\"group\"), with = FALSE]\n```\n:::\n\n\n\n\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlab_data_r <- lab_data\n\nepsilon <-  .08\nminpts <- 200L #should be an integer for the python env\n\n\n\nstime <- Sys.time()\ndb_result_r <- dbscan(lab_data_r, eps = epsilon, minPts = minpts)\netime <- Sys.time()\n\ndelta_t_r <- etime-stime; delta_t_r\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTime difference of 9.155792 secs\n```\n\n\n:::\n\n```{.r .cell-code}\n# result\ntable(db_result_r$cluster)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n    0     1     2 \n  143 99925 99932 \n```\n\n\n:::\n\n```{.r .cell-code}\nlab_data_r$group <- as.factor(db_result_r$cluster)\n\n# visualization\nggplot(lab_data_r, aes(x = x, y = y, color = group)) +\n  geom_point(size = 2) +\n  coord_fixed(ratio = 1) +\n  theme_minimal() +\n  labs(title = \"Randomized Overlapping Semicircles\", x = NULL, y = NULL) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n### Python\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlab_data_py <- lab_data\n\npy$lab_data <- as.matrix(lab_data_py)\npy$epsilon <- epsilon\npy$minpts <- minpts\n```\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nfrom dbscan import DBSCAN\nimport time\n\nprint(\"type(X):\", type(lab_data))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntype(X): <class 'numpy.ndarray'>\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"shape(X):\", getattr(lab_data, 'shape', None))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nshape(X): (200000, 2)\n```\n\n\n:::\n\n```{.python .cell-code}\nstart = time.time()\nlabels, core_samples_mask = DBSCAN(lab_data, eps = epsilon, min_samples = minpts)\nend = time.time()\n\n\ndelta_t = end - start\n\nprint(f\"Elapsed time: {delta_t:.4f} seconds\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nElapsed time: 0.0612 seconds\n```\n\n\n:::\n\n```{.python .cell-code}\n\n\n# 결과를 R로 반환\n#r.labels = labels\n#r.core_mask = core_samples_mask\n\n```\n:::\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# result\ntable(py$labels)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   -1     0     1 \n  143 99925 99932 \n```\n\n\n:::\n\n```{.r .cell-code}\nlab_data_py$group <- as.factor(py$labels+1) # as index number starts from 0\n\n# visualization\nggplot(lab_data_r, aes(x = x, y = y, color = group)) +\n  geom_point(size = 2) +\n  coord_fixed(ratio = 1) +\n  theme_minimal() +\n  labs(title = \"Randomized Overlapping Semicircles\", x = NULL, y = NULL) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n### Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#speedup\n\n\ncat(delta_t_r[[1]]/py$delta_t,\" times speed-up\\n\",sep = \"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n149.6187 times speed-up\n```\n\n\n:::\n:::\n\n\n\n## Some Real World Dataset\n\n\nSource: [NYC Taxi Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmonths <- 1\n\nurls <- paste0(\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-\",sprintf(\"%02d\", 1:months),\".parquet\")\n\n\nplan(multisession,workers = 2)\n\ntaxi_list <- future_lapply(urls, function(url) as.data.table(read_parquet(url))) |> rbindlist()\n\ngc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           used  (Mb) gc trigger  (Mb)  max used  (Mb)\nNcells  2629385 140.5    4683532 250.2   4245902 226.8\nVcells 55371243 422.5  104111586 794.4 102824799 784.5\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(taxi_list)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   VendorID tpep_pickup_datetime tpep_dropoff_datetime passenger_count\n      <int>               <POSc>                <POSc>           <int>\n1:        2  2024-01-01 00:57:55   2024-01-01 01:17:43               1\n2:        1  2024-01-01 00:03:00   2024-01-01 00:09:36               1\n3:        1  2024-01-01 00:17:06   2024-01-01 00:35:01               1\n4:        1  2024-01-01 00:36:38   2024-01-01 00:44:56               1\n5:        1  2024-01-01 00:46:51   2024-01-01 00:52:57               1\n6:        1  2024-01-01 00:54:08   2024-01-01 01:26:31               1\n   trip_distance RatecodeID store_and_fwd_flag PULocationID DOLocationID\n           <num>      <int>             <char>        <int>        <int>\n1:          1.72          1                  N          186           79\n2:          1.80          1                  N          140          236\n3:          4.70          1                  N          236           79\n4:          1.40          1                  N           79          211\n5:          0.80          1                  N          211          148\n6:          4.70          1                  N          148          141\n   payment_type fare_amount extra mta_tax tip_amount tolls_amount\n          <int>       <num> <num>   <num>      <num>        <num>\n1:            2        17.7   1.0     0.5       0.00            0\n2:            1        10.0   3.5     0.5       3.75            0\n3:            1        23.3   3.5     0.5       3.00            0\n4:            1        10.0   3.5     0.5       2.00            0\n5:            1         7.9   3.5     0.5       3.20            0\n6:            1        29.6   3.5     0.5       6.90            0\n   improvement_surcharge total_amount congestion_surcharge Airport_fee\n                   <num>        <num>                <num>       <num>\n1:                     1        22.70                  2.5           0\n2:                     1        18.75                  2.5           0\n3:                     1        31.30                  2.5           0\n4:                     1        17.00                  2.5           0\n5:                     1        16.10                  2.5           0\n6:                     1        41.50                  2.5           0\n```\n\n\n:::\n\n```{.r .cell-code}\n# select numeric rows & filtering out NAs\ntaxi_numeric_dt <- taxi_list[\n  trip_distance > 0 & fare_amount > 0 & !is.na(trip_distance) & !is.na(fare_amount),\n  .(trip_distance, fare_amount)\n]\n\nrm(taxi_list)\ngc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           used  (Mb) gc trigger  (Mb)  max used  (Mb)\nNcells  2635969 140.8    4683532 250.2   4245902 226.8\nVcells 13705004 104.6   83289269 635.5 102824799 784.5\n```\n\n\n:::\n\n```{.r .cell-code}\ntaxi_numeric_dt_200k <- taxi_numeric_dt[sample(.N, 200000)]\n#taxi_numeric_dt_10m <- taxi_numeric_dt[sample(.N, 10000000)]\n\n\npy$taxi_numeric_dt_200k <- as.matrix(taxi_numeric_dt_200k)\n#py$taxi_numeric_dt_10m <- as.matrix(taxi_numeric_dt_10m)\n```\n:::\n\n\n\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nepsilon <-  0.3\nminpts <- 10L\n\n\nstime <- Sys.time()\ndb_result_r <- dbscan(taxi_numeric_dt_200k, eps = epsilon, minPts = minpts)\netime <- Sys.time()\n\ndelta_t_r <- etime-stime; delta_t_r\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTime difference of 11.56615 secs\n```\n\n\n:::\n\n```{.r .cell-code}\ntable(db_result_r$cluster)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     0      1      2      3      4      5      6      7      8      9     10 \n  4389 163774   6336   4349     79     24   1035   6355    206   1185   2048 \n    11     12     13     14     15     16     17     18     19     20     21 \n  1503   1218    270    301    617    893    311    106    145    313    247 \n    22     23     24     25     26     27     28     29     30     31     32 \n   244    161     74     70    224    205    155    155    437    137    449 \n    33     34     35     36     37     38     39     40     41     42     43 \n    39    135     68     76    164     29    167     37     39    118     40 \n    44     45     46     47     48     49     50     51     52     53     54 \n    27     39     18     17     16    142     49     72     44     19     11 \n    55     56     57     58     59     60     61     62     63     64     65 \n    15      7     14     32     14     31     11     10     21     31     12 \n    66     67     68     69     70     71     72     73     74     75     76 \n    11     18     24     20     30     12     11     20     10     33     14 \n    77     78     79     80     81     82     83     84     85     86     87 \n    10     13      9     10     11     16     11     12      9     10      7 \n    88     89     90     91     92     93     94     95     96     97     98 \n    10     10     10      3     10     10      7     10     10     10     10 \n```\n\n\n:::\n:::\n\n\n\n\n### Python\n\n\n::: {.cell}\n\n```{.r .cell-code}\npy$taxi_numeric_dt_200k <- as.matrix(taxi_numeric_dt_200k)\npy$epsilon <- epsilon\npy$minpts <- minpts\n```\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nfrom dbscan import DBSCAN\nimport time\n\nprint(\"type(X):\", type(taxi_numeric_dt_200k))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntype(X): <class 'numpy.ndarray'>\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"shape(X):\", getattr(taxi_numeric_dt_200k, 'shape', None))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nshape(X): (200000, 2)\n```\n\n\n:::\n\n```{.python .cell-code}\nstart = time.time()\nlabels, core_samples_mask = DBSCAN(taxi_numeric_dt_200k, eps=0.3, min_samples=10)\nend = time.time()\n\n\n\ndelta_t = end - start\n\nprint(f\"Elapsed time: {delta_t:.4f} seconds\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nElapsed time: 0.0386 seconds\n```\n\n\n:::\n:::\n\n\n\n### Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count how many rows are assigned to each label, sort the counts in ascending order, and return the difference between the two label distributions\n\n\nsort(as.vector(table(as.factor(py$labels)))) - sort(as.vector(table(as.factor(db_result_r$cluster))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  3  0  1  3  1  1  0  0  0  0  0  0  0  0  0  0  0  0  1  1  0  0  0  0  1\n[26]  1  0  0  1  0  0  0  0  0  0  0  0 -1  0 -1  0  0  0  0  0  1  0  0  0  0\n[51]  1  0  0  0  0  0  0  0  0  0  0  0  0 -1 -2  0  0  0  0  0  0  0  0  0  0\n[76] -3  0  0  0  0  0  0  0  0  0 -1  0  0  0 -2  3  0 -2  0  0  0  0  0 -6\n```\n\n\n:::\n\n```{.r .cell-code}\n# A slight difference is observed, but it appears to stem from floating-point or grid based operations == negligible.\n```\n:::\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#speedup\n\n\ncat(delta_t_r[[1]]/py$delta_t,\" times speed-up\\n\",sep = \"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n299.5211 times speed-up\n```\n\n\n:::\n:::\n\n\n\n\n# Take-home message\n\n## DBSCAN is a slow algorithm\n\n- Worst-case time: $\\Theta(n^2 \\cdot d)$ regardless of indexing.\n- Well-behaved low-dimensional data with effective indexing can approach $O(n \\log n \\cdot d)$.\n- As $d$ or $\\varepsilon$ grow, expect degradation toward quadratic behavior.\n- Tuning $\\varepsilon$, choosing appropriate indexes, and reducing $d$ (e.g., via PCA) often matter more than micro-optimizations.\n- If you cannot apply the above strategies, **use a high-performance implementation**.\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}