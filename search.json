[
  {
    "objectID": "about/blog/index.html",
    "href": "about/blog/index.html",
    "title": "About My Blog",
    "section": "",
    "text": "Source code for this blog\n  \n  \n      Creating a website with quarto\n  \n  \n      Quarto official website"
  },
  {
    "objectID": "posts/dbscan_bench/index.html",
    "href": "posts/dbscan_bench/index.html",
    "title": "How to boost a DBSCAN task",
    "section": "",
    "text": "Chat\n\n\n[Frodo, 2:37 PM] I’m working on finding representative points for census tracts using nationwide residential building data, weighted by the population of each tract. [Frodo, 2:38 PM] But DBSCAN is taking forever lol, so I’m running it in parallel. [Frodo, 2:38 PM] Using something like the future package in R. [Yong-Hun Suh, 2:41 PM] Yeah, DBSCAN’s time complexity isn’t great… [Yong-Hun Suh, 4:47 PM] Are you still working on that (DBSCAN)? [Yong-Hun Suh, 4:47 PM] If it’s a project you’re going to continue, I can give you a tip… [Frodo, 4:53 PM] Just fixed the code and running it now haha, results should come out in a few days if it’s fast.\n\nI was chatting with my colleague from grad school who said DBSCAN was running too slowly on their project.\nHe had already been using a high speed C++ implementation of DBSCAN in R. Still, however, it was giving an eternal waiting.\nI thought, ‘Why not help him out and turn this into a blog article?’\n\n\n\n\n\n\n\nTipWhat is Time Complexity?\n\n\n\nIn theoretical computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. In the plot below, \\(n\\) represents the size of the input, and \\(N\\) represents the number of operations the algorithm performs. This relationship is a critical factor in defining the algorithm’s performance, as the efficiency of the algorithm can be assessed by how \\(N\\) changes as the input size \\(n\\) increases.\nLearn more\n\n\n\nCodelibrary(data.table)\nlibrary(ggplot2)\n\n# Seq `n` gen\nn &lt;- seq(0, 100, by = 0.01)  # need to make the increment small in order to avoid `Inf`s\n\n# data.table\ndf &lt;- data.table(\n                  n = n,\n                  `O(1)` = 1,\n                  `O(log n)` = log2(n),\n                  `O(n)` = n,\n                  `O(n log n)` = n * log2(n),\n                  `O(n^2)` = n^2,\n                  `O(2^n)` = 2^n,\n                  `O(n!)` = factorial(n)\n                )\n\ndf_long &lt;- data.table::melt(df, id.vars = \"n\", variable.name = \"Complexity\", value.name = \"Time\")\n\nggplot(df_long, aes(x = n, y = Time, color = Complexity)) +\n      geom_line(size = 1.2) +\n            ylim(1, 100) +\n            xlim(1, 100)+\n      labs(\n            #title = \"Compirison of Computational Complexity\",\n            x = \"Input Size (n)\",\n            y = \"Number of Operations (N)\",\n            color = \"Complexity\"\n          ) +\n      theme_minimal() +\n      theme(\n            plot.title = element_text(size = 16, face = \"bold\"),\n            legend.title = element_text(size = 12),\n            legend.text = element_text(size = 10)\n      )\n\n\n\nIt is good to know if the algorithm can handle my problem well!\n\n\nCompirison of Computational Complexity\n\n\nDBSCAN’s runtime is dominated by how you perform neighborhood (range) queries. The classic “it’s \\(O(n²)\\)” is only the worst case of a spectrum that depends on data distribution, dimensionality, the index used, and the radius parameter \\(\\varepsilon\\).\n\n\n\n\nRange queries: For each point, find all neighbors within radius \\(\\varepsilon\\). Distance evaluation for one pair costs \\(O(d)\\) in \\(d\\)-dimensional Euclidean space.\n\nCluster expansion: A queue-based flood-fill that repeatedly issues range queries starting from core points (points with at least \\(\\text{minPts}\\) neighbors within \\(\\varepsilon\\)).\n\nAsymptotically, the number and cost of range queries dominate; the expansion logic is linear in the number of discovered neighbors but tied to the same range-query results.\n\n\n\nPer range query: compute distance to all points ⇒ \\(O(n \\cdot d)\\).\nOne query per point in the simplest implementation ⇒ \\(O(n^2 \\cdot d)\\).\nCluster expansion may reuse queries or cause repeats; asymptotically the bound remains \\(\\Theta(n^2 \\cdot d)\\) in the worst case.\n\n\nIndex build: typically \\(O(n \\log n)\\) time, \\(O(n)\\) space.\nRange query cost:\n\nBest/average (well-behaved low–moderate \\(d\\), balanced tree, moderate \\(\\varepsilon\\)): \\(O(\\log n + k)\\), where \\(k\\) is the number of reported neighbors.\nWorst case (high \\(d\\), large \\(\\varepsilon\\), or adversarial data): degenerates to \\(O(n)\\).\n\n\nOverall:\n\nBest/average: \\(O(n \\log n + \\sum \\limits_{i=1}^{n} (\\log n + k_i)) = O(n \\log n + K)\\), where \\(K = \\sum k_i\\) is total neighbor reports. If density per query is bounded, \\(K = O(n)\\), giving \\(O(n \\log n)\\) plus distance cost factor \\(O(d)\\).\nWorst: \\(O(n^2 \\cdot d)\\).\n\n\n\n\nBuild grid hashing once: expected \\(O(n)\\).\nRange query: constant number of adjacent cells, expected \\(O(1 + k)\\) in 2D/3D if \\(\\varepsilon\\) is aligned with cell size and data are not pathologically skewed.\nOverall in practice: near-linear \\(O(n + K)\\), again with distance cost \\(O(d)\\), but this approach becomes brittle as \\(d\\) grows.\n\n\nIn all cases, include the distance computation factor \\(O(d)\\). For high dimensions, tree and grid pruning effectiveness collapses, pushing complexity toward \\(\\Theta(n^2 \\cdot d)\\).\n\n\n\n\nDimensionality \\(d\\): Distance costs scale with \\(O(d)\\), and index pruning degrades with the curse of dimensionality, often turning tree queries into \\(O(n)\\).\n\nNeighborhood radius \\(\\varepsilon\\): Larger \\(\\varepsilon\\) increases average neighbor count \\(k\\), raises \\(K=\\sum k_i\\), and triggers more expansions; in the limit, most points neighbor each other ⇒ near \\(O(n^2 \\cdot d)\\).\n\nData distribution and density: Well-separated, roughly uniform, low-density data favor subquadratic performance with indexes. Dense clusters or large connected components increase expansions and repeats.\n\nminPts: Affects how many points become core (thus how much expansion occurs). It changes constants and practical behavior but not the worst-case big-O bound.\n\nDistance metric: Non-Euclidean metrics can alter pruning efficacy and per-distance cost.\n\nImplementation details: Caching of range queries, deduplication, and “expand only once per point” optimizations materially reduce constants.\n\n\n\n\n\n\n\n\n\nScenario\nAssumptions\nTypical complexity\n\n\n\nBrute-force baseline\nAny \\(d\\), no index\n\\(\\Theta(n^2 \\cdot d)\\)\n\n\nTree index, low–moderate \\(d\\)\n\nBalanced tree, moderate \\(\\varepsilon\\), bounded neighbor counts\n\\(O(n \\log n \\cdot d)\\)\n\n\nTree index, high \\(d\\) or large \\(\\varepsilon\\)\n\nPoor pruning, dense neighborhoods\n\\(\\Theta(n^2 \\cdot d)\\)\n\n\nGrid index (2D/3D)\nWell-chosen cell size, non-pathological data\nNear \\(O(n \\cdot d)\\)\n\n\n\nApproximate NN/range\nANN structure (e.g., HNSW), approximate neighbors\nSubquadratic wall-time; formal bounds vary\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe index build cost \\(O(n \\log n)\\) (trees) or \\(O(n)\\) (grid) is typically amortized across all range queries.\nIf each point’s neighbor count \\(k_i\\) is bounded by a small constant on average, and pruning works, the total neighbor reports \\(K\\) is \\(O(n)\\), leading to near \\(O(n \\log n)\\) behavior with trees.\n\n\n\n\nBut… What if you neither can reduce your dataset… nor change the parameter of the DBSCAN?\n\n\nUse more efficient algorithms that fully leverage your hardware’s full potential!\nFor this case, I used dbscan-python, which is a high-performance parallel implementation of DBSCAN, based on the SIGMOD 2020 paper “Theoretically Efficient and Practical Parallel DBSCAN.”\nThis work achieves theoretically-efficient clustering by minimizing total work and maintaining polylogarithmic parallel depth, enabling scalable performance on large datasets.\nCompared to the naive \\(O(n^2)\\) approach, the proposed algorithm reduces time complexity to \\(O(n \\log n)\\) work and \\(O(\\log n)\\) depth in 2D, and sub-quadratic work in higher dimensions through grid-based partitioning, parallel union-find, and spatial indexing techniques.\nAs the name of the package implies, I need to use Python again for this experiment."
  },
  {
    "objectID": "posts/dbscan_bench/index.html#analyzing-complexity-factors",
    "href": "posts/dbscan_bench/index.html#analyzing-complexity-factors",
    "title": "How to boost a DBSCAN task",
    "section": "",
    "text": "TipWhat is Time Complexity?\n\n\n\nIn theoretical computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. In the plot below, \\(n\\) represents the size of the input, and \\(N\\) represents the number of operations the algorithm performs. This relationship is a critical factor in defining the algorithm’s performance, as the efficiency of the algorithm can be assessed by how \\(N\\) changes as the input size \\(n\\) increases.\nLearn more\n\n\n\nCodelibrary(data.table)\nlibrary(ggplot2)\n\n# Seq `n` gen\nn &lt;- seq(0, 100, by = 0.01)  # need to make the increment small in order to avoid `Inf`s\n\n# data.table\ndf &lt;- data.table(\n                  n = n,\n                  `O(1)` = 1,\n                  `O(log n)` = log2(n),\n                  `O(n)` = n,\n                  `O(n log n)` = n * log2(n),\n                  `O(n^2)` = n^2,\n                  `O(2^n)` = 2^n,\n                  `O(n!)` = factorial(n)\n                )\n\ndf_long &lt;- data.table::melt(df, id.vars = \"n\", variable.name = \"Complexity\", value.name = \"Time\")\n\nggplot(df_long, aes(x = n, y = Time, color = Complexity)) +\n      geom_line(size = 1.2) +\n            ylim(1, 100) +\n            xlim(1, 100)+\n      labs(\n            #title = \"Compirison of Computational Complexity\",\n            x = \"Input Size (n)\",\n            y = \"Number of Operations (N)\",\n            color = \"Complexity\"\n          ) +\n      theme_minimal() +\n      theme(\n            plot.title = element_text(size = 16, face = \"bold\"),\n            legend.title = element_text(size = 12),\n            legend.text = element_text(size = 10)\n      )\n\n\n\nIt is good to know if the algorithm can handle my problem well!\n\n\nCompirison of Computational Complexity"
  },
  {
    "objectID": "posts/dbscan_bench/index.html#dbscan-time-complexity",
    "href": "posts/dbscan_bench/index.html#dbscan-time-complexity",
    "title": "How to boost a DBSCAN task",
    "section": "",
    "text": "DBSCAN’s runtime is dominated by how you perform neighborhood (range) queries. The classic “it’s \\(O(n²)\\)” is only the worst case of a spectrum that depends on data distribution, dimensionality, the index used, and the radius parameter \\(\\varepsilon\\).\n\n\n\n\nRange queries: For each point, find all neighbors within radius \\(\\varepsilon\\). Distance evaluation for one pair costs \\(O(d)\\) in \\(d\\)-dimensional Euclidean space.\n\nCluster expansion: A queue-based flood-fill that repeatedly issues range queries starting from core points (points with at least \\(\\text{minPts}\\) neighbors within \\(\\varepsilon\\)).\n\nAsymptotically, the number and cost of range queries dominate; the expansion logic is linear in the number of discovered neighbors but tied to the same range-query results.\n\n\n\nPer range query: compute distance to all points ⇒ \\(O(n \\cdot d)\\).\nOne query per point in the simplest implementation ⇒ \\(O(n^2 \\cdot d)\\).\nCluster expansion may reuse queries or cause repeats; asymptotically the bound remains \\(\\Theta(n^2 \\cdot d)\\) in the worst case.\n\n\nIndex build: typically \\(O(n \\log n)\\) time, \\(O(n)\\) space.\nRange query cost:\n\nBest/average (well-behaved low–moderate \\(d\\), balanced tree, moderate \\(\\varepsilon\\)): \\(O(\\log n + k)\\), where \\(k\\) is the number of reported neighbors.\nWorst case (high \\(d\\), large \\(\\varepsilon\\), or adversarial data): degenerates to \\(O(n)\\).\n\n\nOverall:\n\nBest/average: \\(O(n \\log n + \\sum \\limits_{i=1}^{n} (\\log n + k_i)) = O(n \\log n + K)\\), where \\(K = \\sum k_i\\) is total neighbor reports. If density per query is bounded, \\(K = O(n)\\), giving \\(O(n \\log n)\\) plus distance cost factor \\(O(d)\\).\nWorst: \\(O(n^2 \\cdot d)\\).\n\n\n\n\nBuild grid hashing once: expected \\(O(n)\\).\nRange query: constant number of adjacent cells, expected \\(O(1 + k)\\) in 2D/3D if \\(\\varepsilon\\) is aligned with cell size and data are not pathologically skewed.\nOverall in practice: near-linear \\(O(n + K)\\), again with distance cost \\(O(d)\\), but this approach becomes brittle as \\(d\\) grows.\n\n\nIn all cases, include the distance computation factor \\(O(d)\\). For high dimensions, tree and grid pruning effectiveness collapses, pushing complexity toward \\(\\Theta(n^2 \\cdot d)\\)."
  },
  {
    "objectID": "posts/dbscan_bench/index.html#what-influences-the-runtime",
    "href": "posts/dbscan_bench/index.html#what-influences-the-runtime",
    "title": "How to boost a DBSCAN task",
    "section": "",
    "text": "Dimensionality \\(d\\): Distance costs scale with \\(O(d)\\), and index pruning degrades with the curse of dimensionality, often turning tree queries into \\(O(n)\\).\n\nNeighborhood radius \\(\\varepsilon\\): Larger \\(\\varepsilon\\) increases average neighbor count \\(k\\), raises \\(K=\\sum k_i\\), and triggers more expansions; in the limit, most points neighbor each other ⇒ near \\(O(n^2 \\cdot d)\\).\n\nData distribution and density: Well-separated, roughly uniform, low-density data favor subquadratic performance with indexes. Dense clusters or large connected components increase expansions and repeats.\n\nminPts: Affects how many points become core (thus how much expansion occurs). It changes constants and practical behavior but not the worst-case big-O bound.\n\nDistance metric: Non-Euclidean metrics can alter pruning efficacy and per-distance cost.\n\nImplementation details: Caching of range queries, deduplication, and “expand only once per point” optimizations materially reduce constants."
  },
  {
    "objectID": "posts/dbscan_bench/index.html#practical-scenarios",
    "href": "posts/dbscan_bench/index.html#practical-scenarios",
    "title": "How to boost a DBSCAN task",
    "section": "",
    "text": "Scenario\nAssumptions\nTypical complexity\n\n\n\nBrute-force baseline\nAny \\(d\\), no index\n\\(\\Theta(n^2 \\cdot d)\\)\n\n\nTree index, low–moderate \\(d\\)\n\nBalanced tree, moderate \\(\\varepsilon\\), bounded neighbor counts\n\\(O(n \\log n \\cdot d)\\)\n\n\nTree index, high \\(d\\) or large \\(\\varepsilon\\)\n\nPoor pruning, dense neighborhoods\n\\(\\Theta(n^2 \\cdot d)\\)\n\n\nGrid index (2D/3D)\nWell-chosen cell size, non-pathological data\nNear \\(O(n \\cdot d)\\)\n\n\n\nApproximate NN/range\nANN structure (e.g., HNSW), approximate neighbors\nSubquadratic wall-time; formal bounds vary\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe index build cost \\(O(n \\log n)\\) (trees) or \\(O(n)\\) (grid) is typically amortized across all range queries.\nIf each point’s neighbor count \\(k_i\\) is bounded by a small constant on average, and pruning works, the total neighbor reports \\(K\\) is \\(O(n)\\), leading to near \\(O(n \\log n)\\) behavior with trees.\n\n\n\n\nBut… What if you neither can reduce your dataset… nor change the parameter of the DBSCAN?"
  },
  {
    "objectID": "posts/dbscan_bench/index.html#solution-at-least-for-this-case",
    "href": "posts/dbscan_bench/index.html#solution-at-least-for-this-case",
    "title": "How to boost a DBSCAN task",
    "section": "",
    "text": "Use more efficient algorithms that fully leverage your hardware’s full potential!\nFor this case, I used dbscan-python, which is a high-performance parallel implementation of DBSCAN, based on the SIGMOD 2020 paper “Theoretically Efficient and Practical Parallel DBSCAN.”\nThis work achieves theoretically-efficient clustering by minimizing total work and maintaining polylogarithmic parallel depth, enabling scalable performance on large datasets.\nCompared to the naive \\(O(n^2)\\) approach, the proposed algorithm reduces time complexity to \\(O(n \\log n)\\) work and \\(O(\\log n)\\) depth in 2D, and sub-quadratic work in higher dimensions through grid-based partitioning, parallel union-find, and spatial indexing techniques.\nAs the name of the package implies, I need to use Python again for this experiment."
  },
  {
    "objectID": "posts/dbscan_bench/index.html#setups",
    "href": "posts/dbscan_bench/index.html#setups",
    "title": "How to boost a DBSCAN task",
    "section": "\n2.1 Setups",
    "text": "2.1 Setups\n\n2.1.1 Used R Packages\n\nCodelibrary(dbscan)\n\n\nAttaching package: 'dbscan'\n\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n\nCodelibrary(data.table)\nlibrary(arrow)\n\n\nAttaching package: 'arrow'\n\n\nThe following object is masked from 'package:utils':\n\n    timestamp\n\nCodelibrary(future.apply)\n\nLoading required package: future\n\nCodelibrary(ggplot2)\n\n\n\n2.1.2 Python Setup\n\nCodelibrary(reticulate)\n\n\n\nif (Sys.info()[[1]]==\"Windows\") {\n        \n    # For my Windows Environment\n    use_condaenv(\"C:/Users/dydgn/miniforge3/envs/dbscan/python.exe\")\n    \n    } else{\n    \n    system(\"micromamba install -n baseline -c conda-forge dbscan -y\")\n\n    # For github actions to utilize CI/CD\n\n    use_condaenv(\"/home/runner/micromamba/envs/baseline/bin/python\", required = TRUE)   \n\n    \n}\n\n\n\nsys &lt;- import(\"sys\")\n\nsys$executable\n\n[1] \"/home/runner/micromamba/envs/baseline/bin/python\""
  },
  {
    "objectID": "posts/dbscan_bench/index.html#generated-lab-dataset",
    "href": "posts/dbscan_bench/index.html#generated-lab-dataset",
    "title": "How to boost a DBSCAN task",
    "section": "\n2.2 Generated Lab Dataset",
    "text": "2.2 Generated Lab Dataset\nFirst, I had to replicate a famous dataset for demonstrating DBSCAN.\n\nCodegenerate_random_semicircle &lt;- function(center_x, center_y, radius, start_angle, end_angle, n_points = 100, noise = 0.08) {\n    \n      angles &lt;- runif(n_points, min = start_angle, max = end_angle)\n      x &lt;- center_x + radius * cos(angles) + rnorm(n_points, 0, noise)\n      y &lt;- center_y + radius * sin(angles) + rnorm(n_points, 0, noise)\n      \n      return(as.data.table(list(x = x, y = y)))\n}\n\n\nset.seed(123)\n\n\nN &lt;- 1e+5L\n\n\nsemicircle1 &lt;- generate_random_semicircle(center_x = 0, center_y = 0.25, radius = 1,\n                                          start_angle = pi, end_angle = 2 * pi, n_points = N)\nsemicircle1[, group := \"Semicircle 1\"]\n\n\nsemicircle2 &lt;- generate_random_semicircle(center_x = 1, center_y = -0.25, radius = 1,\n                                          start_angle = 0, end_angle = pi, n_points = N)\nsemicircle2[, group := \"Semicircle 2\"]\n\n\n\ndf &lt;- rbindlist(list(semicircle1, semicircle2))\n\n\n# visualization\nggplot(df, aes(x = x, y = y, color = group)) +\n  geom_point(size = 2) +\n  coord_fixed(ratio = 1) +\n  theme_minimal() +\n  labs(title = \"Randomized Overlapping Semicircles\", x = NULL, y = NULL) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nCodelab_data &lt;- df[, !c(\"group\"), with = FALSE]\n\n\n\n2.2.1 R - Naive\n\nCodelab_data_r &lt;- lab_data\n\nepsilon &lt;-  .08\nminpts &lt;- 200L #should be an integer for the python env\n\n\n\nstime &lt;- Sys.time()\ndb_result_r &lt;- dbscan(lab_data_r, eps = epsilon, minPts = minpts)\netime &lt;- Sys.time()\n\ndelta_t_r &lt;- etime-stime; delta_t_r\n\nTime difference of 9.05127 secs\n\nCode# result\ntable(db_result_r$cluster)\n\n\n    0     1     2 \n  143 99925 99932 \n\nCodelab_data_r$group &lt;- as.factor(db_result_r$cluster)\n\n# visualization\nggplot(lab_data_r, aes(x = x, y = y, color = group)) +\n    geom_point(size = 2) +\n    coord_fixed(ratio = 1) +\n    theme_minimal() +\n    labs(title = \"Randomized Overlapping Semicircles\", x = NULL, y = NULL) +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n2.2.2 Python - wangyiqiu/dbscan-python\n\n\nCodelab_data_py &lt;- lab_data\n\npy$lab_data &lt;- as.matrix(lab_data_py)\npy$epsilon &lt;- epsilon\npy$minpts &lt;- minpts\n\n\n\nCodeimport numpy as np\nfrom dbscan import DBSCAN\nimport time\n\nprint(\"type(X):\", type(lab_data))\n\ntype(X): &lt;class 'numpy.ndarray'&gt;\n\nCodeprint(\"shape(X):\", getattr(lab_data, 'shape', None))\n\nshape(X): (200000, 2)\n\nCodestart = time.time()\nlabels, core_samples_mask = DBSCAN(lab_data, eps = epsilon, min_samples = minpts)\nend = time.time()\n\n\ndelta_t = end - start\n\nprint(f\"Elapsed time: {delta_t:.4f} seconds\")\n\nElapsed time: 0.0561 seconds\n\nCode\n\n# 결과를 R로 반환\n#r.labels = labels\n#r.core_mask = core_samples_mask\n\n\n\nCode# result\ntable(py$labels)\n\n\n   -1     0     1 \n  143 99925 99932 \n\nCodelab_data_py$group &lt;- as.factor(py$labels+1) # as index number starts from 0\n\n# visualization\nggplot(lab_data_py, aes(x = x, y = y, color = group)) +\n      geom_point(size = 2) +\n      coord_fixed(ratio = 1) +\n      theme_minimal() +\n      labs(title = \"Randomized Overlapping Semicircles\", x = NULL, y = NULL) +\n      theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n2.2.3 Results\n\nCode#speedup\n\n\ncat(delta_t_r[[1]]/py$delta_t,\" times speed-up\\n\",sep = \"\")\n\n161.4497 times speed-up"
  },
  {
    "objectID": "posts/dbscan_bench/index.html#some-real-world-dataset",
    "href": "posts/dbscan_bench/index.html#some-real-world-dataset",
    "title": "How to boost a DBSCAN task",
    "section": "\n2.3 Some Real World Dataset",
    "text": "2.3 Some Real World Dataset\nSource: NYC Taxi Data\n\nCodemonths &lt;- 1\n\nurls &lt;- paste0(\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-\",sprintf(\"%02d\", 1:months),\".parquet\")\n\n\nplan(multisession,workers = 2)\n\ntaxi_list &lt;- future_lapply(urls, function(url) as.data.table(read_parquet(url))) |&gt; rbindlist()\n\ngc()\n\n           used  (Mb) gc trigger  (Mb)  max used  (Mb)\nNcells  2684215 143.4    4876622 260.5   3829681 204.6\nVcells 55665917 424.7  104465195 797.1 103119473 786.8\n\nCodehead(taxi_list)\n\n   VendorID tpep_pickup_datetime tpep_dropoff_datetime passenger_count\n      &lt;int&gt;               &lt;POSc&gt;                &lt;POSc&gt;           &lt;int&gt;\n1:        2  2024-01-01 00:57:55   2024-01-01 01:17:43               1\n2:        1  2024-01-01 00:03:00   2024-01-01 00:09:36               1\n3:        1  2024-01-01 00:17:06   2024-01-01 00:35:01               1\n4:        1  2024-01-01 00:36:38   2024-01-01 00:44:56               1\n5:        1  2024-01-01 00:46:51   2024-01-01 00:52:57               1\n6:        1  2024-01-01 00:54:08   2024-01-01 01:26:31               1\n   trip_distance RatecodeID store_and_fwd_flag PULocationID DOLocationID\n           &lt;num&gt;      &lt;int&gt;             &lt;char&gt;        &lt;int&gt;        &lt;int&gt;\n1:          1.72          1                  N          186           79\n2:          1.80          1                  N          140          236\n3:          4.70          1                  N          236           79\n4:          1.40          1                  N           79          211\n5:          0.80          1                  N          211          148\n6:          4.70          1                  N          148          141\n   payment_type fare_amount extra mta_tax tip_amount tolls_amount\n          &lt;int&gt;       &lt;num&gt; &lt;num&gt;   &lt;num&gt;      &lt;num&gt;        &lt;num&gt;\n1:            2        17.7   1.0     0.5       0.00            0\n2:            1        10.0   3.5     0.5       3.75            0\n3:            1        23.3   3.5     0.5       3.00            0\n4:            1        10.0   3.5     0.5       2.00            0\n5:            1         7.9   3.5     0.5       3.20            0\n6:            1        29.6   3.5     0.5       6.90            0\n   improvement_surcharge total_amount congestion_surcharge Airport_fee\n                   &lt;num&gt;        &lt;num&gt;                &lt;num&gt;       &lt;num&gt;\n1:                     1        22.70                  2.5           0\n2:                     1        18.75                  2.5           0\n3:                     1        31.30                  2.5           0\n4:                     1        17.00                  2.5           0\n5:                     1        16.10                  2.5           0\n6:                     1        41.50                  2.5           0\n\nCode# select numeric rows & filtering out NAs\ntaxi_numeric_dt &lt;- taxi_list[\n    trip_distance &gt; 0 & fare_amount &gt; 0 & !is.na(trip_distance) & !is.na(fare_amount),\n    .(trip_distance, fare_amount)\n]\n\nrm(taxi_list)\ngc()\n\n           used  (Mb) gc trigger  (Mb)  max used  (Mb)\nNcells  2690799 143.8    4876622 260.5   3829681 204.6\nVcells 13999678 106.9   83572156 637.7 103119473 786.8\n\nCodetaxi_numeric_dt_200k &lt;- taxi_numeric_dt[sample(.N, 200000)]\n#taxi_numeric_dt_10m &lt;- taxi_numeric_dt[sample(.N, 10000000)]\n\n\npy$taxi_numeric_dt_200k &lt;- as.matrix(taxi_numeric_dt_200k)\n#py$taxi_numeric_dt_10m &lt;- as.matrix(taxi_numeric_dt_10m)\n\n\n\n2.3.1 R - Naive\n\nCodeepsilon &lt;-  0.3\nminpts &lt;- 10L\n\n\nstime &lt;- Sys.time()\ndb_result_r &lt;- dbscan(taxi_numeric_dt_200k, eps = epsilon, minPts = minpts)\netime &lt;- Sys.time()\n\ndelta_t_r &lt;- etime-stime; delta_t_r\n\nTime difference of 11.44536 secs\n\nCodetable(db_result_r$cluster)\n\n\n     0      1      2      3      4      5      6      7      8      9     10 \n  4389 163774   6336   4349     79     24   1035   6355    206   1185   2048 \n    11     12     13     14     15     16     17     18     19     20     21 \n  1503   1218    270    301    617    893    311    106    145    313    247 \n    22     23     24     25     26     27     28     29     30     31     32 \n   244    161     74     70    224    205    155    155    437    137    449 \n    33     34     35     36     37     38     39     40     41     42     43 \n    39    135     68     76    164     29    167     37     39    118     40 \n    44     45     46     47     48     49     50     51     52     53     54 \n    27     39     18     17     16    142     49     72     44     19     11 \n    55     56     57     58     59     60     61     62     63     64     65 \n    15      7     14     32     14     31     11     10     21     31     12 \n    66     67     68     69     70     71     72     73     74     75     76 \n    11     18     24     20     30     12     11     20     10     33     14 \n    77     78     79     80     81     82     83     84     85     86     87 \n    10     13      9     10     11     16     11     12      9     10      7 \n    88     89     90     91     92     93     94     95     96     97     98 \n    10     10     10      3     10     10      7     10     10     10     10 \n\n\n\n2.3.2 Python - wangyiqiu/dbscan-python\n\n\nCodepy$taxi_numeric_dt_200k &lt;- as.matrix(taxi_numeric_dt_200k)\npy$epsilon &lt;- epsilon\npy$minpts &lt;- minpts\n\n\n\nCodeimport numpy as np\nfrom dbscan import DBSCAN\nimport time\n\nprint(\"type(X):\", type(taxi_numeric_dt_200k))\n\ntype(X): &lt;class 'numpy.ndarray'&gt;\n\nCodeprint(\"shape(X):\", getattr(taxi_numeric_dt_200k, 'shape', None))\n\nshape(X): (200000, 2)\n\nCodestart = time.time()\nlabels, core_samples_mask = DBSCAN(taxi_numeric_dt_200k, eps=0.3, min_samples=10)\nend = time.time()\n\n\n\ndelta_t = end - start\n\nprint(f\"Elapsed time: {delta_t:.4f} seconds\")\n\nElapsed time: 0.0379 seconds\n\n\n\n2.3.3 Results\nThe code below shows the results are valid, as it counts how many rows are assigned to each label, sorts the counts in ascending order, and returns the difference between the two label distributions.\n\nCodesort(as.vector(table(as.factor(py$labels)))) - sort(as.vector(table(as.factor(db_result_r$cluster))))\n\n [1]  3  0  1  3  1  1  0  0  0  0  0  0  0  0  0  0  0  0  1  1  0  0  0  0  1\n[26]  1  0  0  1  0  0  0  0  0  0  0  0 -1  0 -1  0  0  0  0  0  1  0  0  0  0\n[51]  1  0  0  0  0  0  0  0  0  0  0  0  0 -1 -2  0  0  0  0  0  0  0  0  0  0\n[76] -3  0  0  0  0  0  0  0  0  0 -1  0  0  0 -2  3  0 -2  0  0  0  0  0 -6\n\n\nA slight difference is observed. but it appears to stem from floating-point or grid based operations, hence it is negligible.\n\nCode#speedup\n\n\ncat(delta_t_r[[1]]/py$delta_t,\" times speed-up\\n\",sep = \"\")\n\n301.7172 times speed-up\n\n\nWhat a whopping improvement isn’t it?"
  },
  {
    "objectID": "posts/envsetting/index.html",
    "href": "posts/envsetting/index.html",
    "title": "R Dependency Test",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\n\n\n\nSee: https://github.com/quarto-dev/quarto-actions/blob/main/examples/example-03-dependencies.md\n\n\n\n\nlibrary(yaml)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\nlibrary(leafem)\n\n\nTidyverse\n\nread_csv(\"./data/SVI_2000_US.csv\")\n\nRows: 65081 Columns: 82\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (9): STATE_FIPS, CNTY_FIPS, STCOFIPS, TRACT, FIPS, STATE_NAME, STATE_AB...\ndbl (73): G1V1R, G1V2R, G1V3R, G1V4R, G2V1R, G2V2R, G2V3R, G2V4R, G3V1R, G3V...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 65,081 × 82\n   STATE_FIPS CNTY_FIPS STCOFIPS TRACT FIPS  STATE_NAME STATE_ABBR COUNTY  G1V1R\n   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;   &lt;dbl&gt;\n 1 01         001       01001    0201… 0100… Alabama    AL         Autau… 0.127 \n 2 01         001       01001    0202… 0100… Alabama    AL         Autau… 0.227 \n 3 01         001       01001    0203… 0100… Alabama    AL         Autau… 0.0766\n 4 01         001       01001    0204… 0100… Alabama    AL         Autau… 0.0454\n 5 01         001       01001    0205… 0100… Alabama    AL         Autau… 0.0367\n 6 01         001       01001    0206… 0100… Alabama    AL         Autau… 0.152 \n 7 01         001       01001    0207… 0100… Alabama    AL         Autau… 0.11  \n 8 01         001       01001    0208… 0100… Alabama    AL         Autau… 0.0844\n 9 01         001       01001    0209… 0100… Alabama    AL         Autau… 0.138 \n10 01         001       01001    0210… 0100… Alabama    AL         Autau… 0.176 \n# ℹ 65,071 more rows\n# ℹ 73 more variables: G1V2R &lt;dbl&gt;, G1V3R &lt;dbl&gt;, G1V4R &lt;dbl&gt;, G2V1R &lt;dbl&gt;,\n#   G2V2R &lt;dbl&gt;, G2V3R &lt;dbl&gt;, G2V4R &lt;dbl&gt;, G3V1R &lt;dbl&gt;, G3V2R &lt;dbl&gt;,\n#   G4V1R &lt;dbl&gt;, G4V2R &lt;dbl&gt;, G4V3R &lt;dbl&gt;, G4V4R &lt;dbl&gt;, G4V5R &lt;dbl&gt;,\n#   USG1V1P &lt;dbl&gt;, USG1V2P &lt;dbl&gt;, USG1V3P &lt;dbl&gt;, USG1V4P &lt;dbl&gt;, USG1TP &lt;dbl&gt;,\n#   USG2V1P &lt;dbl&gt;, USG2V2P &lt;dbl&gt;, USG2V3P &lt;dbl&gt;, USG2V4P &lt;dbl&gt;, USG2TP &lt;dbl&gt;,\n#   USG3V1P &lt;dbl&gt;, USG3V2P &lt;dbl&gt;, USG3TP &lt;dbl&gt;, USG4V1P &lt;dbl&gt;, USG4V2P &lt;dbl&gt;, …\n\n\n\n\ndata.table\n\nfread(\"./data/SVI_2000_US.csv\") |&gt; head()\n\n   STATE_FIPS CNTY_FIPS STCOFIPS TRACT       FIPS STATE_NAME STATE_ABBR  COUNTY\n        &lt;int&gt;     &lt;int&gt;    &lt;int&gt; &lt;int&gt;      &lt;i64&gt;     &lt;char&gt;     &lt;char&gt;  &lt;char&gt;\n1:          1         1     1001 20100 1001020100    Alabama         AL Autauga\n2:          1         1     1001 20200 1001020200    Alabama         AL Autauga\n3:          1         1     1001 20300 1001020300    Alabama         AL Autauga\n4:          1         1     1001 20400 1001020400    Alabama         AL Autauga\n5:          1         1     1001 20500 1001020500    Alabama         AL Autauga\n6:          1         1     1001 20600 1001020600    Alabama         AL Autauga\n    G1V1R  G1V2R G1V3R  G1V4R  G2V1R  G2V2R  G2V3R  G2V4R  G3V1R  G3V2R  G4V1R\n    &lt;num&gt;  &lt;num&gt; &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;\n1: 0.1268 0.0527 17771 0.1841 0.0782 0.2783 0.2032 0.1018 0.0447 0.0000 0.0000\n2: 0.2270 0.0997 14217 0.3249 0.1215 0.2735 0.3466 0.1176 0.6691 0.0045 0.0092\n3: 0.0766 0.0288 18346 0.1699 0.1338 0.2890 0.1902 0.1130 0.1794 0.0056 0.0317\n4: 0.0454 0.0351 19741 0.1341 0.1510 0.2500 0.1842 0.0560 0.0621 0.0000 0.0310\n5: 0.0367 0.0166 24510 0.0863 0.0682 0.3079 0.1193 0.0654 0.1121 0.0059 0.0246\n6: 0.1521 0.0550 16395 0.2386 0.0938 0.3034 0.2214 0.1101 0.2078 0.0185 0.0000\n    G4V2R  G4V3R  G4V4R  G4V5R USG1V1P USG1V2P USG1V3P USG1V4P USG1TP USG2V1P\n    &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;   &lt;num&gt;   &lt;num&gt;   &lt;num&gt;   &lt;num&gt;  &lt;num&gt;   &lt;num&gt;\n1: 0.2075 0.0090 0.0409 0.0389   0.604   0.541   0.566   0.518  0.591   0.214\n2: 0.0198 0.0544 0.0705 0.0140   0.829   0.837   0.790   0.814  0.904   0.495\n3: 0.0143 0.0141 0.0582 0.0270   0.390   0.211   0.533   0.473  0.367   0.580\n4: 0.0492 0.0181 0.0301 0.0040   0.209   0.306   0.452   0.355  0.264   0.687\n5: 0.0070 0.0182 0.0241 0.0000   0.153   0.061   0.258   0.194  0.067   0.162\n6: 0.3373 0.0182 0.0571 0.0000   0.682   0.566   0.658   0.659  0.706   0.306\n   USG2V2P USG2V3P USG2V4P USG2TP USG3V1P USG3V2P USG3TP USG4V1P USG4V2P\n     &lt;num&gt;   &lt;num&gt;   &lt;num&gt;  &lt;num&gt;   &lt;num&gt;   &lt;num&gt;  &lt;num&gt;   &lt;num&gt;   &lt;num&gt;\n1:   0.675   0.547   0.754  0.609   0.169   0.000  0.000   0.000   0.872\n2:   0.641   0.971   0.850  0.935   0.828   0.061  0.375   0.332   0.592\n3:   0.739   0.473   0.820  0.818   0.488   0.072  0.142   0.477   0.571\n4:   0.456   0.439   0.315  0.440   0.233   0.000  0.002   0.474   0.662\n5:   0.829   0.119   0.421  0.236   0.370   0.081  0.078   0.441   0.512\n6:   0.811   0.644   0.811  0.802   0.527   0.289  0.344   0.000   0.954\n   USG4V3P USG4V4P USG4V5P USG4TP  USTP USG1V1F USG1V2F USG1V3F USG1V4F USG1TF\n     &lt;num&gt;   &lt;num&gt;   &lt;num&gt;  &lt;num&gt; &lt;num&gt;   &lt;num&gt;   &lt;num&gt;   &lt;num&gt;   &lt;num&gt;  &lt;num&gt;\n1:   0.048   0.316   0.945  0.574 0.450       0       0       0       0      0\n2:   0.421   0.532   0.846  0.782 0.854       0       0       0       0      0\n3:   0.070   0.454   0.879  0.682 0.522       0       0       0       0      0\n4:   0.107   0.215   0.000  0.280 0.183       0       0       0       0      0\n5:   0.109   0.159   0.000  0.206 0.078       0       0       0       0      0\n6:   0.109   0.446   0.000  0.302 0.557       0       0       0       0      0\n   USG2V1F USG2V2F USG2V3F USG2V4F USG2TF USG3V1F USG3V2F USG3TF USG4V1F\n     &lt;num&gt;   &lt;num&gt;   &lt;num&gt;   &lt;num&gt;  &lt;num&gt;   &lt;num&gt;   &lt;num&gt;  &lt;num&gt;   &lt;num&gt;\n1:       0       0       0       0      0       0       0      0       0\n2:       0       0       1       0      1       0       0      0       0\n3:       0       0       0       0      0       0       0      0       0\n4:       0       0       0       0      0       0       0      0       0\n5:       0       0       0       0      0       0       0      0       0\n6:       0       0       0       0      0       0       0      0       0\n   USG4V2F USG4V3F USG4V4F USG4V5F USG4TF  USTF Totpop2000 Totalhu G1V1N G1V2N\n     &lt;num&gt;   &lt;num&gt;   &lt;num&gt;   &lt;num&gt;  &lt;num&gt; &lt;num&gt;      &lt;num&gt;   &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:       0       0       0       1      1     1       1879     742   227    46\n2:       0       0       0       0      0     1       1934     758   433    80\n3:       0       0       0       0      0     0       3339    1263   250    42\n4:       0       0       0       0      0     0       4556    1871   207    77\n5:       0       0       0       0      0     0       6040    2277   222    49\n6:       1       0       0       0      1     1       3378    1352   514    90\n   G1V4N G2V1N G2V2N G2V3N G2V4N G3V1N G3V2N G4V1N G4V2N G4V3N G4V4N G4V5N\n   &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1:   226   147   523   339    73    84     0     0   154     6    27    73\n2:   376   235   529   603    74  1294     8     7    15    37    48    27\n3:   362   447   965   549   136   599    17    40    18    17    70    90\n4:   412   688  1139   777    98   283     0    58    92    32    53    18\n5:   326   412  1860   644   143   677    33    56    16    40    53     0\n6:   487   317  1025   689   132   702    58     0   456    22    69     0\n                                     Shape Shape.STArea() Shape.STLength()\n                                    &lt;char&gt;          &lt;num&gt;            &lt;num&gt;\n1: (-86.49001754505039, 32.47712149572407)   0.0009407913       0.15003547\n2: (-86.47336703419953, 32.47430466837865)   0.0003177997       0.09226531\n3:  (-86.4602033958281, 32.47548170589444)   0.0005147620       0.10013704\n4: (-86.44371817489963, 32.47198623491002)   0.0006099415       0.11676753\n5: (-86.42267086235849, 32.45886190833748)   0.0011041460       0.16854121\n6: (-86.47834504936274, 32.44206725458296)   0.0007950631       0.16149683\n\n\n\n\nSimple Features\n\nread_sf(\"./data/test.gpkg\") |&gt; st_geometry() |&gt; plot()\n\n\n\n\n\n\n\n\n\n\nTerra\n\nvect(\"./data/test.gpkg\") |&gt; plot()\n\n\n\n\n\n\n\n\n\n\nThemetic Map(tmap) & leafem\n\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\npark_list &lt;- read_sf(\"./data/test.gpkg\") %&gt;% \n    filter(grepl(\"서울\",소재지지번주소)) %&gt;%\n    select(공원명,소재지지번주소)\n\n{\n    tm_basemap(\"OpenStreetMap.HOT\", group = \"Open Street Map - HOT\")+\n        tm_basemap(\"https://mt1.google.com/vt/lyrs=y&hl=en&z={z}&x={x}&y={y}\", group = \"Google Satellite Imagery w/ label\")+\n        tm_basemap(\"https://mt1.google.com/vt/lyrs=s&hl=en&z={z}&x={x}&y={y}\", group = \"Google Satellite Imagery wo/ label\")+\n        tm_shape(park_list)  +                                  \n        tm_dots(size=.5,\n                fill = \"red\",\n                id=\"공원명\") -&gt; themap\n  \n  \n  \n    tmap_leaflet(themap)|&gt;\n        addMouseCoordinates()\n  \n}\n\nRegistered S3 method overwritten by 'jsonify':\n  method     from    \n  print.json jsonlite"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Yonghuni’s Cyber Home",
    "section": "",
    "text": "How to boost a DBSCAN task\n\n\nUtilizing High Performance Library from Python\n\n\n\nCode\n\n\n\n\n\n\n\n\n\nSep 9, 2025\n\n\nYonghun Suh\n\n\n\n\n\n\n\n\n\n\n\n\nPython Dependency Test\n\n\nTest Case of Handling a Binary Data with Python\n\n\n\nCode\n\n\n\n\n\n\n\n\n\nOct 9, 2024\n\n\nYonghun Suh\n\n\n\n\n\n\n\n\n\n\n\n\nR Dependency Test\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n\n\n\nOct 7, 2024\n\n\nYonghun Suh\n\n\n\n\n\n\n\n\n\n\n\n\nMore Coming… Stay Tuned!\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 6, 2024\n\n\nYonghun Suh\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/update/index.html",
    "href": "posts/update/index.html",
    "title": "More Coming… Stay Tuned!",
    "section": "",
    "text": "More update will soon be followed! Stay tuned!"
  },
  {
    "objectID": "posts/pyenvsetting/index.html",
    "href": "posts/pyenvsetting/index.html",
    "title": "Python Dependency Test",
    "section": "",
    "text": "This example demonstrates how Quarto documents can be rendered in GitHub-Actions CI/CD environment with the required Python/R package dependencies. This need pre-setup - see: my .github/workflows and GitHub Actions for more details.\nThe code below is part of what I used for my Master’s thesis. In my thesis, I used the binary data provided from Korea Meteorological Agency(KMA) which contains radar reflectance of precipitation. Using the code below, I derived the rainfall intensity and the amount of antecedent rainfall and plugged them into my machine learning models."
  },
  {
    "objectID": "posts/pyenvsetting/index.html#import-some-packages",
    "href": "posts/pyenvsetting/index.html#import-some-packages",
    "title": "Python Dependency Test",
    "section": "\n3.1 Import Some Packages",
    "text": "3.1 Import Some Packages\n\nCodeimport sys\nprint(sys.executable)\n\n/home/runner/micromamba/envs/baseline/bin/python\n\n\nThis will show where the python executable binary is.\n\nCodeimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport struct\nimport gzip\n\n\nImporting basic packages"
  },
  {
    "objectID": "posts/pyenvsetting/index.html#handling-various-file-formats",
    "href": "posts/pyenvsetting/index.html#handling-various-file-formats",
    "title": "Python Dependency Test",
    "section": "\n3.2 Handling Various File Formats",
    "text": "3.2 Handling Various File Formats\n\n3.2.1 Get the Size of a Compressed Gunzip, i.e., Raw File\n\nCodefile = \"./data/RDR_HSR_22_20220808/RDR_CMP_HSR_PUB_202208082200.bin.gz\"\n\n\n\n# # Get the size of the binary data file in bytes\nfile_size = os.path.getsize(file)\n\nprint(\"The compressed data file size is {} bytes.\".format(file_size))\n\nThe compressed data file size is 608837 bytes.\n\n\n\n3.2.2 Get the Size of a Binary file\n\nCodedef getuncompressedsize(filename):\n    with open(filename, 'rb') as f:\n        f.seek(-4, 2)\n        return struct.unpack('I', f.read(4))[0]\n\nfile_size = getuncompressedsize(file)\n\nprint(\"The binary data file size is {} bytes.\".format(file_size))\n\nThe binary data file size is 13282434 bytes."
  },
  {
    "objectID": "posts/pyenvsetting/index.html#hmm..-do-you-know-how-to-cook-a-binary-file",
    "href": "posts/pyenvsetting/index.html#hmm..-do-you-know-how-to-cook-a-binary-file",
    "title": "Python Dependency Test",
    "section": "\n3.3 Hmm.. Do you know how to cook a binary file?",
    "text": "3.3 Hmm.. Do you know how to cook a binary file?\nI had to read the manual provided to convert a binary file into so-called raster\n\n\n\n\n\n\n\n\n\n3.3.1 Read a binary as a numpy array\n\nCodeheader_size = 1024\n\n\n# Check if the file size is non-zero\nif file_size == 0:\n    print(\"The binary data file is empty.\")\nelse:\n  f = gzip.GzipFile(file)\n  f.seek(header_size)\n  file_content = f.read()\n  data = np.frombuffer(file_content, dtype=np.short)\n  f.close()\n\n1024\n\n\n\n3.3.2 Reshape the array using the number from the official manual\n\nCode# Reshape the data into a 2D array\ndata = data.reshape(2881, 2305)\n# data needs to be flipped!\ndata = np.flipud(data)\n\ndata.shape\n\n(2881, 2305)\n\n\n\n3.3.3 Initial plotting\n\nCode#### Plotting\n\ndef matplot(x):\n  \n  plt.clf()\n  plt.imshow(x)\n  plt.colorbar()\n  plt.show()\n\n\nmatplot(data)\n\n\n\n\n\n\n\n\n3.3.4 Munging data\nTo use as a feature of the model, I had to change the radar reflectance into the amount of rainfall intensity by using Z-R relationship.\n\nCode\n# Scale factor\n\n#define PUB_OUT   -30000 // Outside of the observed region\n\n#define PUB_IN    -25000 // Unobserved areas within the observed region\n\n#define PUB_MIN   -20000 // Minimum value for representation within the observed area\n\n\ndata = np.where(data&lt;-20000, 0, data)\n\necho = data*0.01\n\n\n\n\n# Z-R Relation\n\nZRa = 148.\n\nZRb = 1.59\n\n# converting dBZ to rain\n\ndef dbz2rain(x):\n\n    rain = (x*0.1 - np.log10(ZRa))/ZRb\n\n    rain = 10**rain\n\n    return rain\n\n\nR = dbz2rain(echo)\n\nR[R&lt;=0.04315743] = 0.0\n\n\nAfter the conversion, then we get the actual amount of rainfall intensity in mm/hr.\n\nCode# Unit: millimeter per hour\n\nmatplot(R)"
  },
  {
    "objectID": "posts/pyenvsetting/index.html#plot-using-rasterplot",
    "href": "posts/pyenvsetting/index.html#plot-using-rasterplot",
    "title": "Python Dependency Test",
    "section": "\n4.1 Plot using raster::plot()",
    "text": "4.1 Plot using raster::plot()\n\nCode#handy R-python interface: reticulate\n\npcp &lt;- py$R\n\nlibrary(raster)\n\nLoading required package: sp\n\nCodetest &lt;- raster(pcp,\n       xmn=(-1121*500),\n       xmx=((2305-1121)*500),\n       ymn=(-1681*500),\n       ymx=(2881-1681)*500,\n       crs = CRS(\"+proj=lcc +lat_1=30 +lat_2=60 +lat_0=38 +lon_0=126 +a=6378138.00 +b=6356752.314 +units=m +no_defs\")\n       )\n\n\nplot(test)"
  },
  {
    "objectID": "posts/pyenvsetting/index.html#plot-using-tmap-package",
    "href": "posts/pyenvsetting/index.html#plot-using-tmap-package",
    "title": "Python Dependency Test",
    "section": "\n4.2 Plot using tmap package",
    "text": "4.2 Plot using tmap package\n\nCodelibrary(tmap)\nlibrary(leafem)\n\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\nCodetest[test==0]=NA\n\n{\n    tm_basemap(\"OpenStreetMap.HOT\", group = \"Open Street Map - HOT\")+\n        tm_basemap(\"https://mt1.google.com/vt/lyrs=y&hl=en&z={z}&x={x}&y={y}\", group = \"Google Satellite Imagery w/ label\")+\n        tm_basemap(\"https://mt1.google.com/vt/lyrs=s&hl=en&z={z}&x={x}&y={y}\", group = \"Google Satellite Imagery wo/ label\")+\n        tm_shape(test)  +                                  \n        tm_raster(\n            col.legend = tm_legend(title = \"Max Rainfall Intensity [mm/hr]\"),\n            col.scale = tm_scale_continuous(\n                values = \"-spectral\",\n                ),\n            group = \"Rainfall\" ) -&gt; themap\n  \n  \n  \n      tmap_leaflet(themap)|&gt;\n          addMouseCoordinates()\n  \n}\n\n\n\n\n\n\n\n4.2.1 Extra: Compairing with the plot from Korea Meteorological Agency"
  },
  {
    "objectID": "about/me/index.html",
    "href": "about/me/index.html",
    "title": "Yonghun Suh",
    "section": "",
    "text": "Email\n  \n  \n      ORCID\n  \n  \n    \n     Github\n  \n  \n    \n     CV\n  \n\n  \n  \n\n\nI’m currently a research intern at the Data Science for Humanity Group at the Max Planck Institute for Security and Privacy, where I apply machine learning and remote sensing to better understand and address the challenges facing our society and environment.\nI am interested in leveraging high performance computing within the domain of Geography, GIScience, machine learning, and remote sensing. My thesis of master degree involves real-time monitoring of landslide susceptibility using high-resolution active remote sensing data.\nI earned my Master’s degree in Geography from Seoul National University, advised by Dr. Gunhak Lee. Prior to that, I completed my undergraduate studies at Kongju National University, earning a B.A. in Geography and a B.Sc. in Atmospheric Science. Throughout my academic journey, I’ve published several research papers and presented at various conferences, exploring topics like landslide prediction, population density estimation, and the impact of air pollution on active transportation.\nIn addition to my research, I’m involved in a community-based air quality project at the State University of New York at Buffalo(UB Clean Air). I’m skilled in programming with R, Python, and Julia, and I have extensive experience with GIS and remote sensing software."
  },
  {
    "objectID": "about/me/index.html#seoul-national-university",
    "href": "about/me/index.html#seoul-national-university",
    "title": "Yonghun Suh",
    "section": "Seoul National University",
    "text": "Seoul National University\n\nSeoul, South Korea | Sept 2021 – Feb 2024\n\nM.A. in Geography\n\nThesis: Real-time Landslide Susceptibility Monitoring Using Spatio-temporal High-resolution Active Remote Sensing Data: An Interpretable Machine Learning Approach (available in Korean)\nAdvised by Dr. Gunhak Lee"
  },
  {
    "objectID": "about/me/index.html#kongju-national-university",
    "href": "about/me/index.html#kongju-national-university",
    "title": "Yonghun Suh",
    "section": "Kongju National University",
    "text": "Kongju National University\n\nSouth Chungcheong Province, South Korea | Mar 2015 – Aug 2021\n\nB.A. in Geography\nB.Sc. in Atmospheric Science"
  },
  {
    "objectID": "about/me/index.html#mapping-reduced-accessibility-to-wash-facilities-in-rohingya-refugee-camps-with-sub-meter-imagery-under-review",
    "href": "about/me/index.html#mapping-reduced-accessibility-to-wash-facilities-in-rohingya-refugee-camps-with-sub-meter-imagery-under-review",
    "title": "Yonghun Suh",
    "section": "Mapping Reduced Accessibility to WASH Facilities in Rohingya Refugee Camps with Sub-Meter Imagery (under review)",
    "text": "Mapping Reduced Accessibility to WASH Facilities in Rohingya Refugee Camps with Sub-Meter Imagery (under review)\n\n\n\n\n\n\n\n\n  \n    \n      \n      \n    \n\n    \n      \n        \n          \n          \n            \n          \n        \n        Accessibility in 2022 (A), 2025 (B), and change (C)\n      \n\n      \n        \n          \n            \n          \n        \n        Method overview\n      \n    \n\n    \n      \n    \n    \n      \n    \n  \n\n\n\n\n\n\nThis study assesses changes in spatial accessibility to WASH (Water, Sanitation, and Hygiene) facilities in Rohingya refugee camps using a 2SFCA approach with shelter detection from sub-meter imagery via a semi-supervised segmentation model, addressing challenges from dense and heterogeneous camp structures.\nAccessibility declined over time, with the lack of gender-segregated facilities revealing persistent accessibility inequality.\nLinks to the Preprint & Source Code"
  },
  {
    "objectID": "about/me/index.html#from-data-to-dissemination-creating-dashboards-for-buffalo-african-american-community-based-participatory-air-monitoring",
    "href": "about/me/index.html#from-data-to-dissemination-creating-dashboards-for-buffalo-african-american-community-based-participatory-air-monitoring",
    "title": "Yonghun Suh",
    "section": "From Data to Dissemination: Creating Dashboards for Buffalo African-American Community-based Participatory Air Monitoring",
    "text": "From Data to Dissemination: Creating Dashboards for Buffalo African-American Community-based Participatory Air Monitoring\n\n\n\n\n\n\n\n\n\n\nBreathe Buffalo: UB Air Quality Monitoring Dashboard\n\n\n\n\n\n\n\nArchitecture Overview\n\n\n\n\n\n\n\n\nThis project deployed 30 low-cost air sensors across East Buffalo to capture local PM2.5 data, revealing pollution patterns overlooked by official monitors.\nA community-designed dashboard improved real-time access to air quality data, supporting environmental awareness and public health equity.\nThe findings were presented at the Joint Annual Meeting of the International Society of Exposure Science and the International Society for Environmental Epidemiology 2025 (ISES-ISEE 2025).\nLinks to the Poster & Source Code"
  },
  {
    "objectID": "about/me/index.html#predicting-landslide-susceptibility-using-high-resolution-active-remote-sensing-data-an-interpretable-machine-learning-approach",
    "href": "about/me/index.html#predicting-landslide-susceptibility-using-high-resolution-active-remote-sensing-data-an-interpretable-machine-learning-approach",
    "title": "Yonghun Suh",
    "section": "Predicting Landslide Susceptibility Using High-resolution Active Remote Sensing Data: An Interpretable Machine Learning Approach",
    "text": "Predicting Landslide Susceptibility Using High-resolution Active Remote Sensing Data: An Interpretable Machine Learning Approach\n\n\n\n\n\n\n\n\nThis study presents a machine learning model to predict landslide susceptibility using Interferometric SAR displacement data and Hybrid Surface Rainfall estimates, emphasizing the role of active remote sensing.\nThe findings highlight the potential of using remote sensing data for landslide susceptibility assessment, offering valuable insights for disaster preparedness and mitigation strategies.\nPublished in Journal of the Korean Cartographic Association with Dr. Gunhak Lee."
  },
  {
    "objectID": "about/me/index.html#delayed-effects-of-air-pollution-on-public-bike-sharing-system-use-in-seoul-south-korea-a-time-series-analysis",
    "href": "about/me/index.html#delayed-effects-of-air-pollution-on-public-bike-sharing-system-use-in-seoul-south-korea-a-time-series-analysis",
    "title": "Yonghun Suh",
    "section": "Delayed effects of air pollution on public bike-sharing system use in Seoul, South Korea: A time series analysis",
    "text": "Delayed effects of air pollution on public bike-sharing system use in Seoul, South Korea: A time series analysis\n\n\n\n\n\n\n\n\nThis study examines the short-term impact of ambient air pollution on public bike-sharing system (PBS) usage in Seoul from 2018 to 2021, finding a consistent negative association between elevated levels of air pollutants (PM2.5, PM10, NO2, and O3) and bike usage the following day.\nThe analysis reveals that high pollution days are linked to a reduction in cycling behavior, with no variations based on age, COVID-19, or seasonality.\nPublished in Social Science & Medicine with Dr. Eun-Hye Yoo and Dr. John E. Roberts."
  },
  {
    "objectID": "about/me/index.html#estimation-of-the-de-facto-population-at-the-building-scale-using-a-dasymetric-mapping-method-based-on-gwr",
    "href": "about/me/index.html#estimation-of-the-de-facto-population-at-the-building-scale-using-a-dasymetric-mapping-method-based-on-gwr",
    "title": "Yonghun Suh",
    "section": "Estimation of the de Facto Population at the Building Scale Using a Dasymetric Mapping Method Based on GWR",
    "text": "Estimation of the de Facto Population at the Building Scale Using a Dasymetric Mapping Method Based on GWR\n\n\n\n\n\n\n\n\nThis study proposes a method using geographically weighted regression(GWR) and dasymetric mapping to estimate fine-scale population in buildings, addressing the challenges of COVID-19 transmission in urban areas.\nBy utilizing building usage and floor area as auxiliary data, it provides detailed population distribution estimates for Seoul, which can inform effective pandemic response strategies.\nPublished in Journal of the Korean Cartographic Association with Dr. Gunhak Lee."
  },
  {
    "objectID": "about/me/index.html#accessibility-analysis-of-public-cooling-shelters-in-seoul-considering-local-temperature-focusing-on-the-elderly-population",
    "href": "about/me/index.html#accessibility-analysis-of-public-cooling-shelters-in-seoul-considering-local-temperature-focusing-on-the-elderly-population",
    "title": "Yonghun Suh",
    "section": "Accessibility Analysis of Public Cooling Shelters in Seoul Considering Local Temperature: Focusing on the Elderly Population",
    "text": "Accessibility Analysis of Public Cooling Shelters in Seoul Considering Local Temperature: Focusing on the Elderly Population\n\n\n\n\n\n\n\n\nThis study aimed to identify vulnerable areas and inform heatwave policy decisions.\nIt examined vulnerability using local Bi-var Moran’s I, analyzing the spatial accessibility of heatwave shelters for the elderly and temperature variations in Seoul.\nThe findings were presented at the 2022 Annual Conference of the Korean Geographical Society."
  }
]